{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://colab.research.google.com/github/bhaavvya/Android_Malware_Detection/blob/onegram_file/OneGramClassifierLSTM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"]},{"cell_type":"markdown","metadata":{"id":"bbZwTXr-RTs3"},"source":["Importing the Libraries"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FRUNacX-FoDm","outputId":"c53a77b4-a36c-4df5-9a96-d7cf3e5c1c87","executionInfo":{"status":"ok","timestamp":1710924495171,"user_tz":-330,"elapsed":51992,"user":{"displayName":"BHAVYA VERMA","userId":"17867056397664606511"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Importing the dataset\n","dataset = pd.read_csv('/content/drive/MyDrive/mp/onegram.csv')"],"metadata":{"id":"G-zKHXIgFqo4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"twhheMOWRKQ2"},"source":["import warnings\n","warnings.filterwarnings(\"ignore\")\n","import numpy as np\n","import pandas as pd\n","from sklearn.pipeline import Pipeline"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Dependent and Independent Variables\n","X = dataset.iloc[:, 1]\n","y = dataset.iloc[:, 2]\n","\n","# Encoding categorical data\n","# Encoding the Independent Variable\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","vectorizer = TfidfVectorizer(ngram_range = (1,1),max_features=1000)\n","X = vectorizer.fit_transform(X)\n","\n","# Encoding the Dependent Variable\n","from sklearn.preprocessing import LabelEncoder\n","labelencoder_y = LabelEncoder()\n","y = labelencoder_y.fit_transform(y)\n","from tensorflow.keras.utils import to_categorical\n","\n","y = to_categorical(y, num_classes=len(labelencoder_y.classes_))\n","# Splitting the dataset into the Training set and Test set\n","from sklearn.model_selection import train_test_split\n","X_train, X_test, y_train, y_test = train_test_split(X.toarray(), y, test_size = 0.25, random_state = 101)\n","\n","# Feature Scaling\n","from sklearn.preprocessing import StandardScaler\n","sc = StandardScaler()\n","X_train = sc.fit_transform(X_train)\n","X_test = sc.transform(X_test)\n","\n","# Fitting RNN to the Training set\n","from keras.layers import Flatten\n","from keras.layers import Dropout\n","from keras.models import Sequential\n","from keras.layers import SimpleRNN,Flatten\n","from keras.layers import Dense\n","\n","#from keras.utils.vis_utils import plot_model\n","\n","x_train = X_train.reshape(-1, 1, X_train.shape[1])\n","model = Sequential()\n","\n","# Adding RNN layer and some Dropout regularisation\n","model.add(SimpleRNN(100,return_sequences=True,input_shape=(x_train.shape[1],x_train.shape[2])))\n","#Adding a Dropout Layer\n","model.add(Dropout(0.5))  #To Prevent Overfitting\n","#Adding a Flatten Layer\n","model.add(Flatten())     #Flattens input. To preserve weight ordering when switching from one data format to another data format.\n","#Adding a Dense Layer (Deeply connected layer)\n","model.add(Dense(100, activation='relu'))   #All inputs and outputs are connected , Rectified Linear Unit as Activation Function\n","model.add(Dense(len(labelencoder_y.classes_), activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam'   #Back Propagation, Weight Adjustment\n","              , metrics=['accuracy'])\n","history = model.fit(x_train, y_train, epochs=100, batch_size=128)\n","print(model.summary())\n","\n","# Predicting the Test set results\n","y_pred = np.argmax(model.predict(X_test.reshape(-1, 1, X_test.shape[1])) ,axis=1)\n","y_test = np.argmax(y_test,axis=1)\n","\n","#Accuracy Score\n","from sklearn import metrics\n","print(\"Accuracy Score: \", metrics.accuracy_score(y_test, y_pred))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pFmrOojWIBe9","executionInfo":{"status":"ok","timestamp":1710925835033,"user_tz":-330,"elapsed":18562,"user":{"displayName":"BHAVYA VERMA","userId":"17867056397664606511"}},"outputId":"a5b5991f-8c01-43fb-ff40-baf5ebeab3c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","17/17 [==============================] - 3s 6ms/step - loss: 1.1601 - accuracy: 0.4958\n","Epoch 2/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.7371 - accuracy: 0.7366\n","Epoch 3/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.6247 - accuracy: 0.7812\n","Epoch 4/100\n","17/17 [==============================] - 0s 11ms/step - loss: 0.5538 - accuracy: 0.8174\n","Epoch 5/100\n","17/17 [==============================] - 0s 9ms/step - loss: 0.5235 - accuracy: 0.8371\n","Epoch 6/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.5011 - accuracy: 0.8296\n","Epoch 7/100\n","17/17 [==============================] - 0s 12ms/step - loss: 0.4729 - accuracy: 0.8498\n","Epoch 8/100\n","17/17 [==============================] - 0s 11ms/step - loss: 0.4631 - accuracy: 0.8507\n","Epoch 9/100\n","17/17 [==============================] - 0s 9ms/step - loss: 0.4531 - accuracy: 0.8573\n","Epoch 10/100\n","17/17 [==============================] - 0s 11ms/step - loss: 0.4371 - accuracy: 0.8549\n","Epoch 11/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.4270 - accuracy: 0.8690\n","Epoch 12/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.4154 - accuracy: 0.8620\n","Epoch 13/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3972 - accuracy: 0.8653\n","Epoch 14/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.4120 - accuracy: 0.8714\n","Epoch 15/100\n","17/17 [==============================] - 0s 9ms/step - loss: 0.3944 - accuracy: 0.8714\n","Epoch 16/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.3875 - accuracy: 0.8676\n","Epoch 17/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.3788 - accuracy: 0.8756\n","Epoch 18/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3669 - accuracy: 0.8789\n","Epoch 19/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3654 - accuracy: 0.8770\n","Epoch 20/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3509 - accuracy: 0.8779\n","Epoch 21/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3505 - accuracy: 0.8803\n","Epoch 22/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.3537 - accuracy: 0.8765\n","Epoch 23/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.3348 - accuracy: 0.8826\n","Epoch 24/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.3431 - accuracy: 0.8822\n","Epoch 25/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3203 - accuracy: 0.8944\n","Epoch 26/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3357 - accuracy: 0.8854\n","Epoch 27/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.3189 - accuracy: 0.8892\n","Epoch 28/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3187 - accuracy: 0.8920\n","Epoch 29/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3096 - accuracy: 0.8948\n","Epoch 30/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.3173 - accuracy: 0.8939\n","Epoch 31/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.3039 - accuracy: 0.8972\n","Epoch 32/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.3030 - accuracy: 0.8911\n","Epoch 33/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2995 - accuracy: 0.8901\n","Epoch 34/100\n","17/17 [==============================] - 0s 9ms/step - loss: 0.2938 - accuracy: 0.8977\n","Epoch 35/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2966 - accuracy: 0.9000\n","Epoch 36/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2921 - accuracy: 0.8972\n","Epoch 37/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2931 - accuracy: 0.8986\n","Epoch 38/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.2822 - accuracy: 0.9038\n","Epoch 39/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2843 - accuracy: 0.9038\n","Epoch 40/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2783 - accuracy: 0.8991\n","Epoch 41/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2813 - accuracy: 0.9014\n","Epoch 42/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2744 - accuracy: 0.9070\n","Epoch 43/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2687 - accuracy: 0.9052\n","Epoch 44/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2592 - accuracy: 0.9103\n","Epoch 45/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2696 - accuracy: 0.9089\n","Epoch 46/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.2584 - accuracy: 0.9089\n","Epoch 47/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2596 - accuracy: 0.9042\n","Epoch 48/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2556 - accuracy: 0.9080\n","Epoch 49/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2460 - accuracy: 0.9080\n","Epoch 50/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2483 - accuracy: 0.9028\n","Epoch 51/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2509 - accuracy: 0.9131\n","Epoch 52/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.2527 - accuracy: 0.9094\n","Epoch 53/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2571 - accuracy: 0.9113\n","Epoch 54/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2434 - accuracy: 0.9150\n","Epoch 55/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2438 - accuracy: 0.9131\n","Epoch 56/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2386 - accuracy: 0.9164\n","Epoch 57/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2343 - accuracy: 0.9183\n","Epoch 58/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.2343 - accuracy: 0.9183\n","Epoch 59/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2279 - accuracy: 0.9169\n","Epoch 60/100\n","17/17 [==============================] - 0s 9ms/step - loss: 0.2313 - accuracy: 0.9197\n","Epoch 61/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2270 - accuracy: 0.9244\n","Epoch 62/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2342 - accuracy: 0.9188\n","Epoch 63/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2379 - accuracy: 0.9080\n","Epoch 64/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2240 - accuracy: 0.9197\n","Epoch 65/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2241 - accuracy: 0.9174\n","Epoch 66/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2160 - accuracy: 0.9225\n","Epoch 67/100\n","17/17 [==============================] - 0s 11ms/step - loss: 0.2272 - accuracy: 0.9178\n","Epoch 68/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.2262 - accuracy: 0.9197\n","Epoch 69/100\n","17/17 [==============================] - 0s 13ms/step - loss: 0.2246 - accuracy: 0.9136\n","Epoch 70/100\n","17/17 [==============================] - 0s 8ms/step - loss: 0.2191 - accuracy: 0.9164\n","Epoch 71/100\n","17/17 [==============================] - 0s 10ms/step - loss: 0.2140 - accuracy: 0.9249\n","Epoch 72/100\n","17/17 [==============================] - 0s 7ms/step - loss: 0.2130 - accuracy: 0.9225\n","Epoch 73/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.2113 - accuracy: 0.9239\n","Epoch 74/100\n","17/17 [==============================] - 0s 12ms/step - loss: 0.2121 - accuracy: 0.9225\n","Epoch 75/100\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2078 - accuracy: 0.9249\n","Epoch 76/100\n","17/17 [==============================] - 0s 17ms/step - loss: 0.2006 - accuracy: 0.9277\n","Epoch 77/100\n","17/17 [==============================] - 0s 15ms/step - loss: 0.2218 - accuracy: 0.9202\n","Epoch 78/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2077 - accuracy: 0.9216\n","Epoch 79/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2087 - accuracy: 0.9291\n","Epoch 80/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2072 - accuracy: 0.9272\n","Epoch 81/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1974 - accuracy: 0.9258\n","Epoch 82/100\n","17/17 [==============================] - 0s 4ms/step - loss: 0.1997 - accuracy: 0.9258\n","Epoch 83/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.2002 - accuracy: 0.9282\n","Epoch 84/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1907 - accuracy: 0.9296\n","Epoch 85/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1959 - accuracy: 0.9272\n","Epoch 86/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1935 - accuracy: 0.9249\n","Epoch 87/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.1957 - accuracy: 0.9272\n","Epoch 88/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1911 - accuracy: 0.9286\n","Epoch 89/100\n","17/17 [==============================] - 0s 4ms/step - loss: 0.1971 - accuracy: 0.9277\n","Epoch 90/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1970 - accuracy: 0.9305\n","Epoch 91/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1946 - accuracy: 0.9202\n","Epoch 92/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1931 - accuracy: 0.9296\n","Epoch 93/100\n","17/17 [==============================] - 0s 6ms/step - loss: 0.1808 - accuracy: 0.9338\n","Epoch 94/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1898 - accuracy: 0.9310\n","Epoch 95/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1968 - accuracy: 0.9277\n","Epoch 96/100\n","17/17 [==============================] - 0s 5ms/step - loss: 0.1800 - accuracy: 0.9357\n","Epoch 97/100\n","17/17 [==============================] - 0s 3ms/step - loss: 0.1904 - accuracy: 0.9310\n","Epoch 98/100\n","17/17 [==============================] - 0s 3ms/step - loss: 0.1846 - accuracy: 0.9343\n","Epoch 99/100\n","17/17 [==============================] - 0s 3ms/step - loss: 0.1820 - accuracy: 0.9338\n","Epoch 100/100\n","17/17 [==============================] - 0s 3ms/step - loss: 0.1920 - accuracy: 0.9249\n","Model: \"sequential_3\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," simple_rnn (SimpleRNN)      (None, 1, 100)            16600     \n","                                                                 \n"," dropout_3 (Dropout)         (None, 1, 100)            0         \n","                                                                 \n"," flatten_3 (Flatten)         (None, 100)               0         \n","                                                                 \n"," dense_6 (Dense)             (None, 100)               10100     \n","                                                                 \n"," dense_7 (Dense)             (None, 4)                 404       \n","                                                                 \n","=================================================================\n","Total params: 27104 (105.88 KB)\n","Trainable params: 27104 (105.88 KB)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n","None\n","23/23 [==============================] - 0s 2ms/step\n","Accuracy Score:  0.9169014084507042\n"]}]}]}